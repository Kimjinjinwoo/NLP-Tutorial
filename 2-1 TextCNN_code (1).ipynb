{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-1 TextCNN_code.ipynb","provenance":[],"authorship_tag":"ABX9TyOmqUXeKwquVozrYQlxMnM7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vhRT_wKXhnJ","executionInfo":{"status":"ok","timestamp":1658469568462,"user_tz":-540,"elapsed":7843,"user":{"displayName":"김진우","userId":"12971138652345879224"}},"outputId":"6cb19258-d06d-433f-cf97-f16f98211673"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1000 cost = 0.003749\n","Epoch: 2000 cost = 0.000654\n","Epoch: 3000 cost = 0.000218\n","Epoch: 4000 cost = 0.000092\n","Epoch: 5000 cost = 0.000043\n","sorry hate you is Bad Mean...\n"]}],"source":["# code by Tae Hwan Jung @graykode\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class TextCNN(nn.Module):\n","    def __init__(self):\n","        super(TextCNN , self).__init__()\n","        self.num_filters_total = num_filters * len(filter_sizes)\n","        self.W = nn.Embedding(vocab_size , embedding_size)\n","        self.Weight = nn.Linear(self.num_filters_total , num_classes , bias = False)\n","        self.Bias = nn.Parameter(torch.ones([num_classes]))\n","        self.filter_list = nn.ModuleList([nn.Conv2d(1 , num_filters , (size, embedding_size)) for size in filter_sizes])\n","\n","    def forward(self , X):\n","        embedded_chars = self.W(X) # [batch_size , sequence_length , embedding_size] <- 오타 였을 듯???\n","        embedded_chars = embedded_chars.unsqueeze(1) # add channel (=1) [batch , channel(=1) , sequence_length , embedding_size]\n","\n","        pooled_outputs = []\n","        for i, conv in enumerate(self.filter_list):\n","            # conv : [input_channel(=1), output_channel(=3), (filter_height , filter_width) , bias_option]\n","            h = F.relu(conv(embedded_chars))\n","            # mp : ((filter_height , filter_width))\n","            mp = nn.MaxPool2d((sequence_length - filter_sizes[i] + 1 , 1))\n","            # pooled : [batch_size(=6) , output_height(=1) , output_width(=1) , output_channel(=3)]\n","            pooled = mp(h).permute( 0 , 3 , 2, 1)\n","            pooled_outputs.append(pooled)\n","        \n","        h_pool = torch.cat(pooled_outputs , len(filter_sizes))\n","        h_pool_flat = torch.reshape(h_pool , [-1 , self.num_filters_total])\n","        model = self.Weight(h_pool_flat) + self.Bias\n","        return model\n","\n","if __name__ == '__main__':\n","    embedding_size = 2  # embedding size\n","    sequence_length = 3  # sequence length\n","    num_classes = 2 # number of classes\n","    filter_sizes = [2, 2, 2]    # n-gram windows\n","    num_filters = 3 #number of filters\n","\n","    # 3 words sentences (=sequenct_length is 3)\n","    sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n","    labels = [1, 1, 1, 0, 0, 0] # 1 is good, 0 is not good.\n","\n","    word_list = \" \".join(sentences).split()\n","    word_list = list(set(word_list))\n","    word_dict = {w: i for i, w in enumerate(word_list)}\n","    vocab_size = len(word_dict)\n","\n","    model = TextCNN()\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters() , lr = 0.001)\n","\n","    inputs = torch.LongTensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n","    targets = torch.LongTensor([out for out in labels]) # To using Torch Softmax Loss function\n","\n","    # Training\n","    for epoch in range(5000):\n","        optimizer.zero_grad()\n","        output = model(inputs)\n","\n","        # output : [batch_size , num_classes] , target_batch : [batch_size] (LongTensor , not one-hot)\n","        loss = criterion(output , targets)\n","        if (epoch + 1) % 1000 == 0:\n","            print('Epoch:' , '%04d' % (epoch + 1), 'cost =', '{:6f}'.format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Test\n","    test_text = 'sorry hate you'\n","    tests = [np.asarray([word_dict[n] for n in test_text.split()] )]\n","    test_batch = torch.LongTensor(tests)\n","\n","    # Predict\n","    predict = model(test_batch).data.max(1 , keepdim=True)[1]\n","    if predict[0][0] == 0:\n","        print(test_text , \"is Bad Mean...\")\n","    else:\n","        print(test_text,\"is Good Mean!!\")"]},{"cell_type":"code","source":[""],"metadata":{"id":"PQDAubdkalTF"},"execution_count":null,"outputs":[]}]}