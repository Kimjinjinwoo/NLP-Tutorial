{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq_with_Attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMD4ewxevzx+0HNnfRaqA10"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RgnB190SVU5C","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"ok","timestamp":1660094922854,"user_tz":-540,"elapsed":19576,"user":{"displayName":"김진우","userId":"12971138652345879224"}},"outputId":"4bb848c2-161f-4f65-dec1-0e1bf425479d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0400 cost = 0.000469\n","Epoch: 0800 cost = 0.000149\n","Epoch: 1200 cost = 0.000073\n","Epoch: 1600 cost = 0.000043\n","Epoch: 2000 cost = 0.000028\n","ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","# S: Symbol that shows starting of decoding input\n","# E: Symbol that shows ending of decoding output (starting 으로 되 있었는데 오탄듯)\n","# P: Symbol that will fill in blank sequence if current batch data size is shorter than time steps\n","\n","def make_batch():\n","    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n","    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n","    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n","\n","    # make tensor\n","    return torch.FloatTensor(input_batch) , torch.FloatTensor(output_batch) , torch.LongTensor(target_batch)\n","\n","class Attention(nn.Module):\n","    def __init__(self):\n","        super(Attention, self).__init__()\n","        self.enc_cell = nn.RNN(input_size = n_class , hidden_size = n_hidden , dropout = 0.5)\n","        self.dec_cell = nn.RNN(input_size = n_class , hidden_size = n_hidden , dropout = 0.5)\n","\n","        # Linear for attention\n","        self.attn = nn.Linear(n_hidden , n_hidden)\n","        self.out = nn.Linear(n_hidden * 2 , n_class)\n","\n","    def forward(self, enc_inputs , hidden , dec_inputs):\n","        enc_inputs = enc_inputs.transpose(0,1) # enc_inputs: [n_step(=n_step , time step) , batch_size , n_class]\n","        dec_inputs = dec_inputs.transpose(0,1) # dec_inputs: [n_step(=n_step , time step) , batch_size , n_class]\n","\n","        # enc_outputs : [n_step, batch_size , num_directions(=1) * n_hidden] , matrix F (???)\n","        # enc_hidden : [num_layers(=1) * num_directions(=1) , batch_size , n_hidden]\n","        enc_outputs , enc_hidden = self.enc_cell(enc_inputs , hidden)\n","\n","        trained_attn = []\n","        hidden = enc_hidden\n","        n_step = len( dec_inputs )\n","        model = torch.empty( [ n_step , 1 , n_class ] )\n","\n","        for i in range(n_step): # each time step\n","            # dec_output : [n_step(=1) , batch_size(=1) , num_direcitons(=1) * n_hidden]\n","            # hidden : [num_layers(=1) * num_directions(=1) , batch_size(=1) , n_hidden]\n","            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0) , hidden)\n","            attn_weights = self.get_att_weight(dec_output , enc_outputs)    # attn_weights : [1, 1 , n_step]\n","            trained_attn.append(attn_weights.squeeze().data.numpy())\n","\n","            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step , n_hidden] = [1,1,n_hidden]\n","            context = attn_weights.bmm(enc_outputs.transpose(0,1))\n","            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1) , num_directions(=1) * n_hidden]\n","            context = context.squeeze(1)    #[1 , num_directions(=1) * n_hidden]\n","            model[i] = self.out(torch.cat((dec_output , context) , 1))\n","        \n","        return model.transpose(0, 1).squeeze(0), trained_attn\n","    \n","    def get_att_weight(self, dec_output , enc_outputs): # get attention weight one 'dec_outputs' with 'enc_outputs'\n","        n_step = len(enc_outputs)\n","        attn_scores = torch.zeros(n_step)   # attn_scores : [n_step]\n","\n","        for i in range(n_step):\n","            attn_scores[i] = self.get_att_score(dec_output , enc_outputs[i])\n","\n","        # Normalize scores to weights in range 0 to 1\n","        return F.softmax(attn_scores).view(1,1,-1)\n","\n","    def get_att_score(self, dec_output , enc_output):\n","        score = self.attn(enc_output)\n","        return torch.dot(dec_output.view(-1) , score.view(-1))\n","\n","if __name__ == '__main__':\n","    n_step = 5  # number of cells(= number of Step)\n","    n_hidden = 128  # number of hidden units in one cell\n","\n","    sentences = ['ich mochte ein bier P' , 'S i want a beer' , 'i want a beer E']\n","\n","    word_list = \" \".join(sentences).split()\n","    word_list = list(set(word_list))\n","    word_dict = { w : i for i,w in enumerate(word_list)}\n","    number_dict = { i : w for i,w in enumerate(word_list)}\n","    n_class = len(word_dict)    #vocab list\n","\n","    # hidden : [num_layers(=1) * num_directions(=1) , batch_size , n_hidden ]\n","    hidden = torch.zeros(1 , 1 , n_hidden)\n","\n","    model = Attention()\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters() , lr = 0.001)\n","\n","    input_batch , output_batch , target_batch = make_batch()\n","\n","    # Train\n","    for epoch in range(2000):\n","        optimizer.zero_grad()\n","        output , _ = model(input_batch , hidden , output_batch)\n","\n","        loss = criterion( output , target_batch.squeeze(0) )\n","        if (epoch +1) % 400 == 0:\n","            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","        \n","        loss.backward()\n","        optimizer.step()\n","\n","    # Test\\\n","    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n","    test_batch = torch.FloatTensor(test_batch)\n","    predict , trained_attn = model(input_batch , hidden , test_batch)\n","    predict = predict.data.max(1 , keepdim = True)[1]\n","    print(sentences[0], '->' , [number_dict[n.item()] for n in predict.squeeze()])\n","\n","    # Show Attention\n","    fig = plt.figure(figsize=(5, 5))\n","    ax = fig.add_subplot(1,1,1)\n","    ax.matshow(trained_attn , cmap='viridis')\n","    ax.set_xticklabels([''] + sentences[0].split() , fontdict = {'fontsize': 14})\n","    ax.set_yticklabels([''] + sentences[2].split() , fontdict = {'fontsize': 14})\n","    plt.show()"]},{"cell_type":"code","source":[""],"metadata":{"id":"WqHDNfIjWNOD"},"execution_count":null,"outputs":[]}]}